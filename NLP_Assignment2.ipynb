{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOifg2KXPnczSunToDSXI0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hegame1998/NLP-Assignment/blob/main/NLP_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "xq8_Mm37rOre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I started by importing the **requests** library and then loaded my input texts directly from GitHub using their raw file URLs. These are:\n",
        "\n",
        "* **source_text.txt →** the document I want to summarize.\n",
        "\n",
        "* **style_text.txt →** the document that gives the style I want to follow.\n",
        "\n",
        "After fetching the files, I saved them locally in my Colab workspace so I could reuse or inspect them later if needed."
      ],
      "metadata": {
        "id": "kIsfWFBGrSfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collection & Import Library\n",
        "import requests\n",
        "\n",
        "# Replace with actual raw GitHub URLs\n",
        "source_url = \"https://raw.githubusercontent.com/hegame1998/NLP-Assignment/main/source_text.txt\"\n",
        "style_url = \"https://raw.githubusercontent.com/hegame1998/NLP-Assignment/main/style_text.txt\"\n",
        "\n",
        "# Load text from GitHub\n",
        "source_text = requests.get(source_url).text\n",
        "style_text = requests.get(style_url).text\n",
        "\n",
        "# Save the input texts (optional for reference or reuse)\n",
        "with open(\"source_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(source_text)\n",
        "\n",
        "with open(\"style_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(style_text)\n"
      ],
      "metadata": {
        "id": "T_jfXSiEyYPX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing (without NLTK)"
      ],
      "metadata": {
        "id": "jCsp4tIgyyIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because I didn’t want to rely on external libraries like **nltk.punkt** (which may fail to download), I wrote a simple sentence tokenizer using regular expressions. It splits the text at punctuation marks followed by capital letters — a basic but effective approach."
      ],
      "metadata": {
        "id": "0seOaD7hy2d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing (no punkt)\n",
        "def naive_sentence_tokenize(text):\n",
        "    import re\n",
        "    # Split sentences on punctuation followed by a space and capital letter\n",
        "    return re.split(r'(?<=[.?!])\\s+(?=[A-Z])', text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    sentences = naive_sentence_tokenize(text)\n",
        "    return [s.strip() for s in sentences if len(s.strip()) > 0]"
      ],
      "metadata": {
        "id": "DlP6_yCey1lO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "GUkFCtgpzUdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To manage summarization within a limited context window (e.g., 4000 tokens or fewer), I calculated the proportional space each text should get based on their lengths. This way, I ensured fairness in how much summary content each document contributes."
      ],
      "metadata": {
        "id": "e3nf3MwuzW5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_target_lengths(len1, len2, max_token=4000):\n",
        "    total = len1 + len2\n",
        "    proportion1 = len1 / total\n",
        "    proportion2 = len2 / total\n",
        "    return int(max_token * proportion1), int(max_token * proportion2)"
      ],
      "metadata": {
        "id": "aL8Z8rGtzZ7X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training (Summarization Logic)"
      ],
      "metadata": {
        "id": "4abkQTh0zirr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I built a **hierarchical summarization pipeline**. First, I divided the documents into smaller chunks. Then, I extracted a few key sentences from each chunk. I kept summarizing in this way until I reached the desired summary length.\n",
        "\n",
        "My summarization method is extractive, so I simply picked the first N sentences from each chunk."
      ],
      "metadata": {
        "id": "TSwQMNd6znTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization Logic\n",
        "def hierarchical_summarize(sentences, target_len, slice_size=20):\n",
        "    summary = []\n",
        "    for i in range(0, len(sentences), slice_size):\n",
        "        chunk = sentences[i:i + slice_size]\n",
        "        chunk_summary = simple_extract_summary(chunk, target_len)\n",
        "        summary.extend(chunk_summary)\n",
        "        if len(summary) >= target_len:\n",
        "            break\n",
        "    return summary[:target_len]\n",
        "\n",
        "def simple_extract_summary(sentences, max_sentences):\n",
        "    # Simple extractive summarization: pick first N sentences\n",
        "    return sentences[:max_sentences]"
      ],
      "metadata": {
        "id": "vqEyHJn-zq5C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "WHagCFxOz7iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate my summaries, I printed the number of sentences in both the original and the summarized versions. I also displayed a few lines from the generated summary so I could visually assess the quality."
      ],
      "metadata": {
        "id": "iBk_qKOpz9oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "def evaluate_summary(original, summary, label):\n",
        "    print(f\"=== {label} Summary Evaluation ===\")\n",
        "    print(f\"Original sentences: {len(original)}\")\n",
        "    print(f\"Summary sentences: {len(summary)}\")\n",
        "    print(\"Sample summary:\")\n",
        "    print(\"\\n\".join(summary[:5]))\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "_DtdCM1Tz_NB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Pipeline"
      ],
      "metadata": {
        "id": "7oo6eiCD0KME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I brought everything together into a single function. This function runs preprocessing, computes proportional lengths, generates hierarchical summaries, and evaluates the results."
      ],
      "metadata": {
        "id": "HsBlLNUK0LvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "\n",
        "def main_pipeline(source_text, style_text):\n",
        "    # Preprocessing\n",
        "    source_sentences = preprocess_text(source_text)\n",
        "    style_sentences = preprocess_text(style_text)\n",
        "\n",
        "    # Proportional length calculation\n",
        "    source_target_len, style_target_len = compute_target_lengths(\n",
        "        len(source_sentences), len(style_sentences), max_token=50\n",
        "    )\n",
        "\n",
        "    # Hierarchical summarization\n",
        "    source_summary = hierarchical_summarize(source_sentences, source_target_len)\n",
        "    style_summary = hierarchical_summarize(style_sentences, style_target_len)\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate_summary(source_sentences, source_summary, \"Source\")\n",
        "    evaluate_summary(style_sentences, style_summary, \"Style\")\n",
        "\n",
        "    return source_summary, style_summary\n",
        "\n",
        "# Run the full pipeline\n",
        "source_summary, style_summary = main_pipeline(source_text, style_text)"
      ],
      "metadata": {
        "id": "682nlAEF0N6x",
        "outputId": "6055802a-43f4-47dd-e827-1a0fd94a8413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Source Summary Evaluation ===\n",
            "Original sentences: 60\n",
            "Summary sentences: 45\n",
            "Sample summary:\n",
            "Natural Language Processing (NLP) is a sub-field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
            "The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\n",
            "Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
            "Applications of NLP include speech recognition, text summarization, machine translation, sentiment analysis, and more.\n",
            "The field of NLP combines computational linguistics with statistical, machine learning, and deep learning models.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Style Summary Evaluation ===\n",
            "Original sentences: 6\n",
            "Summary sentences: 4\n",
            "Sample summary:\n",
            "In the beginning, language was simple.\n",
            "It served only to convey the most basic of messages—danger, food, shelter.\n",
            "As human societies grew more complex, so too did their methods of communication.\n",
            "Writing systems emerged, stories were told, philosophies debated.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}