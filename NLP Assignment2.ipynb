{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLttsYk3luEAANMi9El6bM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hegame1998/NLP-Assignment/blob/main/NLP%20Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will do this approach in this code:\n",
        "\n",
        "\n",
        "* **Loads two documents:** one to summarize, one as style reference.\n",
        "\n",
        "* **Estimates token length** using word count (proxy for 4000-token limit).\n",
        "\n",
        "* **Performs chunk-based summarization** using TextRank-style TF-IDF cosine similarity.\n",
        "\n",
        "* **If the summary is too large**, it recursively shrinks it.\n",
        "\n",
        "* **Saves the summaries.**\n",
        "\n",
        "* **Prints a query prompt** to generate a style-following summary."
      ],
      "metadata": {
        "id": "oEUtBIJj_XKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Collection"
      ],
      "metadata": {
        "id": "-bYgasEGDHiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where I load my input and style reference documents.\n"
      ],
      "metadata": {
        "id": "7LXniapmDbse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6dF7HNjEWI5",
        "outputId": "2c60d5ca-dd05-4133-808f-201711714d0f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Data Collection ===\n",
        "def load_documents(input_path, style_path):\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        input_text = f.read()\n",
        "\n",
        "    with open(style_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        style_reference_text = f.read()\n",
        "\n",
        "    return input_text, style_reference_text"
      ],
      "metadata": {
        "id": "WV4CiV-sDHJv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "lrSiUDQRDSM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean and tokenize the text."
      ],
      "metadata": {
        "id": "Rgrd9ocpDhRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Preprocessing ===\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def estimate_token_count(text):\n",
        "    return len(text.split())"
      ],
      "metadata": {
        "id": "IPoeqT2oDVjO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction"
      ],
      "metadata": {
        "id": "1xV4brfYDY3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use TF-IDF and cosine similarity to rank sentences for summarization.\n"
      ],
      "metadata": {
        "id": "Zpf3mDt5Dkgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Feature Extraction ===\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def summarize_text(text, num_sentences=5):\n",
        "    sentences = tokenize_sentences(text)\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return text\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    sim_matrix = cosine_similarity(X)\n",
        "\n",
        "    scores = sim_matrix.sum(axis=1)\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[-num_sentences:]]\n",
        "    ranked_sentences.sort(key=lambda s: sentences.index(s))\n",
        "    return ' '.join(ranked_sentences)\n"
      ],
      "metadata": {
        "id": "ElxMj4FeDnJz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training / Hierarchical Summarization"
      ],
      "metadata": {
        "id": "_jhXgByhDsYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteratively summarize large texts to fit the context window (e.g., 4000 tokens)."
      ],
      "metadata": {
        "id": "TyephVpGDvg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Model Training / Hierarchical Summarization ===\n",
        "TOKEN_LIMIT = 4000\n",
        "\n",
        "def hierarchical_summarize(text, target_token_limit=TOKEN_LIMIT):\n",
        "    clean = clean_text(text)\n",
        "    sentences = tokenize_sentences(clean)\n",
        "    token_estimate = estimate_token_count(clean)\n",
        "\n",
        "    while token_estimate > target_token_limit:\n",
        "        chunks = []\n",
        "        chunk = []\n",
        "        chunk_tokens = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_tokens = estimate_token_count(sent)\n",
        "            if chunk_tokens + sent_tokens > target_token_limit:\n",
        "                chunks.append(' '.join(chunk))\n",
        "                chunk = [sent]\n",
        "                chunk_tokens = sent_tokens\n",
        "            else:\n",
        "                chunk.append(sent)\n",
        "                chunk_tokens += sent_tokens\n",
        "\n",
        "        if chunk:\n",
        "            chunks.append(' '.join(chunk))\n",
        "\n",
        "        summaries = [summarize_text(chunk, num_sentences=5) for chunk in chunks]\n",
        "        sentences = []\n",
        "        for s in summaries:\n",
        "            sentences.extend(tokenize_sentences(s))\n",
        "        text = ' '.join(sentences)\n",
        "        token_estimate = estimate_token_count(text)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "pH9dTZGDDuma"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "AYddWpftD1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output and save summaries, simulate a style-following prompt."
      ],
      "metadata": {
        "id": "v_NqhIKzD3K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Evaluation ===\n",
        "def generate_styled_summary(input_text, style_reference_text):\n",
        "    print(\"→ Measuring document lengths...\")\n",
        "    len_input = estimate_token_count(input_text)\n",
        "    len_style = estimate_token_count(style_reference_text)\n",
        "\n",
        "    print(f\"→ Input tokens: {len_input}, Style reference tokens: {len_style}\")\n",
        "\n",
        "    print(\"→ Summarizing input text hierarchically...\")\n",
        "    summarized_input = hierarchical_summarize(input_text, TOKEN_LIMIT)\n",
        "\n",
        "    print(\"→ Summarizing style reference text hierarchically...\")\n",
        "    summarized_style = hierarchical_summarize(style_reference_text, TOKEN_LIMIT)\n",
        "\n",
        "    with open(\"summarized_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(summarized_input)\n",
        "\n",
        "    with open(\"summarized_style.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(summarized_style)\n",
        "\n",
        "    print(\"\\n--- STYLE-FOLLOWING QUERY EXAMPLE ---\")\n",
        "    print(\"Please summarize the following text using the style of this reference text.\")\n",
        "    print(\"\\nSTYLE SAMPLE:\\n\", summarized_style[:300] + \"...\")\n",
        "    print(\"\\nTEXT TO SUMMARIZE:\\n\", summarized_input[:300] + \"...\")\n"
      ],
      "metadata": {
        "id": "tMBYUPYPD6pb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Function"
      ],
      "metadata": {
        "id": "y2WTpIhXD-le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tie everything together:\n"
      ],
      "metadata": {
        "id": "wqOZD7IKEBKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Entry Point ===\n",
        "if __name__ == \"__main__\":\n",
        "    input_text, style_reference_text = load_documents(\"input_document.txt\", \"style_document.txt\")\n",
        "    generate_styled_summary(input_text, style_reference_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "HhacIpOsEC1s",
        "outputId": "3c23f5a1-b6da-4b4e-9a0a-0058f8e056d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'input_document.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2612289664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# === Entry Point ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_reference_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_document.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"style_document.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgenerate_styled_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_reference_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-3735382548.py\u001b[0m in \u001b[0;36mload_documents\u001b[0;34m(input_path, style_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# === Data Collection ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input_document.txt'"
          ]
        }
      ]
    }
  ]
}